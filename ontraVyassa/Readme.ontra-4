'One Network to Rule them all' (aka ONTRA)
Revised in 2021-10 after Aditya produces 13 exitatory cells x 5 clones


Steps necessary and  data preparation for training of ONTRA-4
Works with 13 excitatory cells x 5 clones,  e-types: cAD

Those are the cell types:
bbp205	L6_TPC_L1_cADpyr231
bbp208	L4_SS_cADpyr230  Note, this one was not in Vyassa's list
bbp102	L4_SP_cADpyr230
bbp207	L6_UTPC_cADpyr231
bbp054	L23_PC_cADpyr229
bbp155	L5_UTPC_cADpyr232
bbp152	L5_STPC_cADpyr232
bbp176	L6_IPC_cADpyr231
bbp206	L6_TPC_L4_cADpyr231
bbp156	L6_BPC_cADpyr231
bbp153	L5_TTPC1_cADpyr232
bbp154	L5_TTPC2_cADpyr232
bbp098	L4_PC_cADpyr230


See also:
https://docs.google.com/document/d/1eOAI2nTTduELE489cuIDe5qIwzXzQn1L9ENlcv2FYwA/edit

https://bitbucket.org/balewski/pitchforkoracle/src/master/docs/Readme.ontra-4


a)  procure list of cells to be averaged & infered
E.g. use master3 spreadsheet and run: rdCellTable.py 
For  all 13 cells are:

cd ~/neuronInverter/ontraVyassa>
$  ./uparCalib.py  --cellName  bbp0545 bbp0985 bbp1025 bbp1525 bbp1535 bbp1545 bbp1555 bbp1565 bbp1765 bbp2055 bbp2065 bbp2075 bbp2085



a.1) add some output to ontra4_excite.conf.yaml

    
For each e-type the physical ranges of conductances were the same.

c) determin linear transformation u --> u*  which will adjust unit-parameters to represent the same physical value for each conductance. See Fig 1a,b in gDoc

Let is j-cell index, k-conductance index, i-frame index

u(j,i,k) has uniform distribution in [-1,1] but the mapping to physical conductances is different for each j-cell:
p(j,i,k)= base(j,k)*10^u(j,i,k)

For fixed k-condutance the following is computed
*) Let a_k=min(base(j,k), b=max(base(j,k)  - min/max over all cells
*) define:
   log10P_jk=log10(base(j,k))
   centP_k= (a_k+b_k)/2.
   delP_k=  (b_k-a_k)/2+1

*) During re-packing u --> u* transormation is as follows
  u*(j,i,k) = ( u(j,i,k) +log10P_jk - contP_k ) / delP_k

*) after prediction, the inverse transformations are:
 u(j,i,k) = u*(j,i,k) * delP  -log10P_jk + contP_k
 p(j,i,k)= base(j,k)*10^u(j,i,k)

The base values for skipped conductances can be recovered from metadata:
ontra4_excite.conf.yaml

*) Quicker method to recover phys conductances from u*:
ontra4_excite.conf.yaml  contains:  centP_k,  delP_k
for [idx, name, centP,delP] in ['conductName']:
    p_k= 10^( u*/delP_k +centP_k)
    p_k=np.exp(( u*/delP_k +centP_k)*np.log(10))


d) identify 4 probes indices   - skip here, see previous iteration.
I'll get  list of 4 probes per cell from Roy - I hope

Criterium:  dist2soma is closest to 50um for soma_dendrites, 150um for axon, apic_dend 150um:
probeNamePerCell:
  bbp054: [[0, 'soma_0', 8.3], [35, 'axon_8', 138.2], [6, 'apic_12', 134.6], [40, 'dend_12', 48.3]]
  bbp098: [[0, 'soma_0', 9.9], [35, 'axon_2', 149.1], [48, 'apic_6', 149.5], [38, 'dend_70', 45.0]]

d.1) add it to ontra4_excite.conf.yaml

e) decide all avaliable frames per cell will be used, the train/val/test data split is done in-fly and stored in to a single h5 file named

???etype_3inhib/bbp019_3inhib.cellSpike_3prB8kHz.data.h5

train_frames                   Dataset {73728, 1600, 3}
train_phys_par                 Dataset {73728, 19}
train_unit_par                 Dataset {73728, 19}
val_xxx (all 3)
test_xx  (all 3) etc.

The meta-file will have the same core name:
bbp019_8inhib.cellSpike_3prB8kHz.meta.yaml

f) decide outPath: ??/global/cfs/cdirs/m2043/balewski/neuronBBP-pack8kHzRam/probe_4prB8kHz/ontra4/etype_excite_v1/

g) input data are 'oryginal': 67pr, 40 kHz: neuronBBP2-data_67pr/

h) all the above decisions are stored in: ontra4_excite.conf.yaml

******* Execute transformation ****
for each cell, it taks 10-15 min for ~700k frames

cd ~/neuronInverter/ontraVyassa
./ontraTransformOne.py --cellName bbp1531

OR use batch: (in misc)
batchOntraTransform.slr
executed like this:
pitchforkOracle> sleep 3; sbatch misc/batchOntraTransform.slr bbp098

i) prep meta-data yaml file  for the training.
- must be stored w/ data (hd5 - files location)
- the name is: meta.cellSpike_excite_4prB8kHz.yaml
- use --probeType excite_4prB8kHz
- assembled manually


*****  run test training on 1 node on CPUs *******
module load tensorflow/gpu-2.1.0-py37
mkdir out
./train_CellSpike.py   --localSamples 36000 --noHorovod

It will open ~90 hd5 files and read only 3400/90=40 samples per file.

The (meaningless) training for 5 epochs will last for ~10 minutes.

*****  run test training on 1 cori-gpu node on 8 GPUs *******
   module load esslurm
   module load tensorflow/gpu-2.1.0-py37
Allocate whole node:
 salloc -N1 -C gpu --ntasks-per-node=8 -c 10  --gres=gpu:8  --exclusive -t4:00:00

The maximal number of events per GPU on Cori-GPU nodes is 1.8M. This means on 1 cgpu node you can only read in 8*1.8M=14M samples. The whole data set has ~45M samples.
Execute:
  srun -n8   bash -c '  nvidia-smi -l 3 >&L.smi_`hostname` & python -u  ./train_CellSpike.py   --localSamples 1800000 --epochs 4 '

By default only rank0 is printing, the other 7 are silent.
You should see loading 1.8M samples takes about 80 seconds:

 IG:load-end of all HD5, read time=76.05(sec) name=0-train-practice numCell=90, numLocSamp=1800000, numLocSamp/cell=20000
 IG:Xall (1800000, 1600, 3) float32
 IG:Uall (1800000, 19) float32

Th emodel size is: Total params: 3,281,579

The local BS is 1k, global BS is 8k

The training speed is: 3.6 min/epoch
Epoch 00002: 
1757/1757 - 231s - loss: 0.0755 - val_loss: 0.0670

*****  PREDICTING *******
To predict for the test-data, based on completed training, do:
 ./predict_CellSpike.py  -n 5000 --seedModel out
 


****** BATCH TRAINING ON 24 GPUs ********
This is the only way to get the training converge. It takes about 3 wall-hours, the end val-loss is of 0.032. The local BS is 1k, global BS is 24k.
trained model & plots are saved in out/ directory

The training should stop automatically around epoch 50, before reaching 60 epochs due to non-improving validation loss.

The local file : L.smi_cgpu02 (depending on the worker name) will show GPU load.
This pdf shows the training history
display out/cellSpike_train_f10.png

submit this job :
   module load esslurm
  sbatch batchTrainSimple.slr 

Output model will be saved in outGpu/

Predicting is quick, (30 sec), execute this command on cori login node (after you load modules)

 ./predict_CellSpike.py  -n 5000 --seedModel outGpu