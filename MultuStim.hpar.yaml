# Neur-inver  config for  BBP3 training, serialized stims  (each stim is an independent sample)
#
# choose data path depending on facility

data_path:
  #perlmutter:  /pscratch/sd/b/balewski/tmp_bbp3_feb22/
  perlmutter:  /pscratch/sd/k/ktub1999/bbp_May_08_19_Wide_1_NoNoise/

data_conf:
    serialize_stims :  True  #Adds each stim as a seperate Sample 
    append_stim : False #Append Stim one after the other
    parallel_stim : False #Parallel stim Only with Multi Model

    #max_glob_samples_per_epoch: 50000  # optional, reduces epoch size
    num_data_workers: 4  

max_epochs: 12  
batch_size: 512

const_local_batch: True   # True is faster  but LR changes w/ num GPUs
validation_period: [1, 1] # [ period, len] (epochs)
#>>> validation is computed if  epoch%period<len

log_freq_per_epoch: 3
tb_show_graph: True

save_checkpoint: False  # only when loss improves
resume_checkpoint: False  # False: always start over 
# warning: for multi-gpu & resume --> val_loss explodes - no loop over GPUs
  
# APEX: Nvidia streamlined data-parallel  training
# AMP: Automatic Mixed Precision package
# autotune: # activates cudnn.benchmark
do_ray: False



opt_pytorch:  
    amp: False
    apex:  False
    autotune:  False
    zerograd: False

train_conf:
    warmup_epochs: 10
    optimizer: [adam, 0.005] # initLR  
    LRsched: { plateau_patience: 8, reduceFactor: 0.11  }

model:
    myId:  m8lay_vassa
    comment: 1-stim 2021-10 not optimized ML model
    num_cnn_blocks: 1
    conv_block: # CNN params
        filter: [90, 120, 30]
        kernel: [ 6, 4, 3]
        pool:   [ 2, 3, 6]

    instance_norm_slot: -9
    layer_norm: False
    batch_norm_cnn_slot: 3
    batch_norm_flat: True  # was former batch_norm

    fc_block: # FC params w/o last layer
       dims: [1024, 768, 256, 512, 128]
       dropFrac: 0.02
