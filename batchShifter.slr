#!/bin/bash -l
# common job script for CoriGpu and Perlmutter using Shifter
#SBATCH --time=30:00  -J ni_h5
#-SBATCH -C gpu -A m2043_g  -q debug # Kris
#SBATCH -C gpu  
#-SBATCH -C gpu -A nstaff_g  -q early_science # for early_science
#SBATCH --image=nersc/pytorch:ngc-21.08-v2
#SBATCH -o out/slurm-%j.out
#-SBATCH  -x cgpu08 # block sick nodes
#---CORI_GPU---
#SBATCH  -N1 --ntasks-per-node=4  --gpus-per-task=1  --cpus-per-task=10
#-SBATCH -N8 --ntasks-per-node=8 --gpus-per-task=1 --cpus-per-task=10 --exclusive
#---PERLMUTTER---
#-SBATCH -N16 --ntasks-per-node=4 --gpus-per-task=1 --cpus-per-task=32 --exclusive
# - - - E N D    O F    SLURM    C O M M A N D S
nprocspn=${SLURM_NTASKS_PER_NODE}
#nprocspn=1  # special case for partial use of full node

#cellName=practice140c  # has 21M training samples, inhibitory
#cellName=witness17c # has 2.5M training samples, inhibitory
#cellName=october12c # has 5.8M training samples, inhibitory
#cellName=practice10c  # has 4.8M training samples, excitatory
#cellName=practice50c  # has 29M training samples, excitatory2
#cellName=witness2c # has 1M training samples, excitatory
cellName=bbp1543  # has 0.5M training samples, inhibitory
design=expF2us  # 'expF2us' is optimal for experiment
numProbe=4
epochs=211 # 50 cells
epochs=141 # 1 cells


N=${SLURM_NNODES}
G=$[ $N * $nprocspn ]
export MASTER_ADDR=`hostname`
export MASTER_PORT=8881
echo S: job=${SLURM_JOBID} MASTER_ADDR=$MASTER_ADDR  MASTER_PORT=$MASTER_PORT  G=$G  N=$N 
nodeList=$(scontrol show hostname $SLURM_NODELIST)
echo S:node-list $nodeList

# grab some variables from environment - if defined
[[ -z "${NEUINV_INIT_LR}" ]] && initLRstr="  " || initLRstr=" --initLR ${NEUINV_INIT_LR} "
[[ -z "${NEUINV_WRK_SUFIX}" ]] && wrkSufix=$SLURM_JOBID || wrkSufix="${NEUINV_WRK_SUFIX}"
[[ -z "${NEUINV_JOBID}" ]] && jobId=j$SLURM_JOBID || jobId="${design}_${NEUINV_JOBID}"
env |grep NEUINV

#wrkSufix=lrScan/$jobId

if [[  $NERSC_HOST == cori ]]   ; then
    echo "S:on Cori-GPU"
    facility=corigpu

elif [[  $NERSC_HOST == perlmutter ]]   ; then
    echo "S:on Perlmutter"
    facility=perlmutter
    # bash -c 'printf "#include <stdio.h>\nint main() {  cudaFree(0);printf(\"cudaFree-done\"); }" > dummy.cu && nvcc -o dummy.exe dummy.cu'
    #  opening and closing a GPU context on each node to reset GPUs
    time srun -n$N -l --ntasks-per-node=1 toolbox/dummy.exe
fi

#wrkDir0=$SCRATCH/tmp_digitalMind/neuInv/september/
wrkDir0=/global/cfs/cdirs/mpccc/balewski/tmp_neurInv2/${cellName}_${numProbe}pr/

wrkDir=$wrkDir0/$wrkSufix

echo "S:cellName=$cellName  initLRstr=$initLRstr jobId=$jobId  wrkSufix=$wrkSufix wrkDir=$wrkDir" 
date

export CMD=" python -u   train_dist.py --cellName $cellName   --facility $facility  --outPath ./out --design $design --jobId $jobId  $initLRstr --numInpChan $numProbe --epochs $epochs "

echo CMD=$CMD

codeList="  train_dist.py  predict.py  toolbox/ batchShifter.slr  $design.hpar.yaml  "

outPath=$wrkDir/out
mkdir -p $outPath
cp -rp $codeList  $wrkDir
cd  $wrkDir
echo lsfPWD=`pwd`

echo "starting  jobId=$jobId neurInv 2021-08 " `date` " outPath= $outPath"
time srun -n $G  shifter  bash  toolbox/driveOneTrain.sh  >& log.train

sleep 3

echo S:done train
time srun -n1 shifter   ./predict.py --numSamples 500000 --modelPath out/ --cellName $cellName -X  >& log.predict
echo S:done predict
date
